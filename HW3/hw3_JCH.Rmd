---
title: "Homework 3, STATS 315A"
subtitle: "Stanford University, Winter 2019"
author: "Joe Higgins"
header-includes:
   - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include = FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
# if you don't have a package, you can get it with "install.packages()"
require('MASS')
require('ggplot2')
require('GGally')
require('dplyr')
require('reshape2')
require('locfdr')
require('mvtnorm')
require('car')
require('Boom')
require('bestglm')
require('BMS')
require('class')
require('glmnet')
require('readr')
require('caret')
require('spline')
t.test.p <- function(x,y) {
  tst = t.test(x,y)
  return(tst$p.value)
}

```

## Question 5

You have a set of $p$ variables in the matrix $X$ which represent some economic indicators measured over a number of years, and a response $y$ also measured for the same years. You fit a linear model, but are criticized because you are told that the regime changes in time, and so should your model. You decide to let your regression coefficients (and intercept) change smoothly with time, a so-called varying coefficient model.

(a) Describe how you could achieve this using natural cubic splines.

(b) Read in the data in `vcdata.csv`. There is a two-column $x$ and a time variable $t$. Fit the varying coefficient model, and plot the three coefficient (functions) versus time (on the same plot using matplot).

```{r}
rm(list = ls())
data <- read.csv("vcdata.csv")
df <- 5

t_splines <- ns(data$t, df = df)
int <- rep(1, nrow(data))
model <- lm(data$y ~ t_splines:(int + data$x.1 + data$x.2))

bint_t <- t_splines[,] %*% model$coefficients[-1][0*df+1:df] + model$coefficients['(Intercept)']
b1_t <-   t_splines[,] %*% model$coefficients[-1][1*df+1:df]
b2_t <-   t_splines[,] %*% model$coefficients[-1][2*df+1:df]

y_hat <- bint_t + (b1_t * data$x.1) + (b2_t * data$x.2)
#y_hat - model$fitted.values
RSS <- (1/nrow(data)) * sum((data$y - y_hat)^2)

coeff_t <- matrix(c(bint_t, b1_t, b2_t), nrow=nrow(data),ncol=(3))
       
matplot(data$t, coeff_t)
```

(c) Compute the pointwise standard errors for each of these, and in- clude a standard-error band (upper and lower) for each function.

```{r}
std_errs <- summary(model)$coefficients[,2]
bint_se <- sum(std_errs[(0*df+1):(df+1)])
b1_se <-   sum(std_errs[1*df+2:(df+1)])
b2_se <-   sum(std_errs[2*df+2:(df+1)])
```

(d) Is this an interaction model? If so, what order.

Yes this is an interaction model. It is a second-order interaction model, since the covariates of the model are based on two vectors multiplied element-wise on each other.


## Question 6

A company in Chile uses crowd-sourcing to fund loans to the public, as a means to offer relief from the high bank interest rates. The data in this challenge consists of historical loan records for a case-control sam- ple of 3000 past customers. The variables characterize some aspects of the loan, such as duration, amount, interest rate and many other more technical features of the loans. There are also some qualitative variables such as reason for the loan, a quality score and so on. One of the variables — our response — is ”default”, a 0/1 variable indicating whether or not the borrower has defaulted on their loan payments.

The company would like to build a default risk score so that they can target high-risk customers early and perhaps preempt the default event, which ends up costly for all involved.

The default rate experienced by this company is 7%. You are provided with a training set `loan_train.csv` which represents a sample of 1000 defaulters, and 2000 non-defaulters, and contains 30 features and the binary outcome ”default” (in the first column). There is also a file `loan_testx.csv` which consists of a random sample of 10000 other customers from the general pool. For these you are provided only the 30 features.

Your job is to build a risk score — probability of default — for each customer in the test set. You may use any of the tools discussed in the lectures in this class. You may not use tools not discussed in this class, such as deep learning, random forests or boosting. You should produce a writeup describing what you did, and how you selected your final model. Give some indication which variables were important in the calculation of your risk score. You will also submit a simple file with 10000 lines, and on each line is your predicted risk estimate for each test customer, in the same order as in `loan_testx.csv.`


```{r}
rm(list = ls())
get_accuracy <- function(y_hat, y){
  correct <- y_hat == y
  pct_correct <- sum(correct)/length(correct)
  return(pct_correct)
}

data_to_matrix <- function(data_frame){
  output <- matrix(, nrow=dim(data_frame)[1], ncol=dim(data_frame)[2])
  for(i in 1:ncol(data_frame)){
    output[,i] <- data_frame[,i]
  }
  return(output)
}

probs_to_classes <- function(probs, p){
  probs[probs > p] <- 1
  probs[probs <= p] <- 0
  return(probs)
}

#import data
train <- read.csv("loan_train.csv")

#create folds
num_folds = 10
flds <- createFolds(train$default, k = num_folds, list = TRUE, returnTrain = FALSE)

#cross validation split
train_valid_split <- function(k, flds){
  tv <- list()
  tv[[1]] <- unlist(flds[-k], use.names = FALSE)
  tv[[2]] <- flds[[k]]
  names(tv) <- c("train", "valid")
  return(tv)
}

#run model through cross validation
for(i in 1:num_folds){
  tv_split <- train_valid_split(i, flds)
  train_x <- train[tv_split$train, 2:dim(train)[2]]
  train_y <- train[tv_split$train, 1]
  valid_x <- train[tv_split$valid, 2:dim(train)[2]]
  valid_y <- train[tv_split$valid, 1]
  
  #model <- glmnet(data_to_matrix(train_x), train_y, family=c("binomial"), alpha = 0.3)
  model <- glm(train_y ~ credit_ratio + interest + amount + term, train_x, family=binomial())
  model_output <- unname(predict(model, valid_data, type="response"))
  y_hat <- probs_to_classes(model_output, 0.50)
  accuracy <- get_accuracy(y_hat, valid_data$default)
  accuracy
}






```